{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bdeeae9a-678f-40bd-8ffc-cd25e6bfb430",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "297df2d4-af0c-4a30-8b1e-6da3fcae23f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from models/llama-2-7b-chat.Q8_0.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 7\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q8_0:  226 tensors\n",
      "llm_load_vocab: special tokens cache size = 259\n",
      "llm_load_vocab: token to piece cache size = 0.1684 MB\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
      "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q8_0\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 6.67 GiB (8.50 BPW) \n",
      "llm_load_print_meta: general.name     = LLaMA v2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.15 MiB\n",
      "llm_load_tensors:        CPU buffer size =  6828.64 MiB\n",
      "...................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   256.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    70.50 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.context_length': '4096', 'general.name': 'LLaMA v2', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '11008', 'llama.attention.layer_norm_rms_epsilon': '0.000001', 'llama.rope.dimension_count': '128', 'llama.attention.head_count': '32', 'tokenizer.ggml.bos_token_id': '1', 'llama.block_count': '32', 'llama.attention.head_count_kv': '32', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '7'}\n",
      "Using fallback chat format: llama-2\n"
     ]
    }
   ],
   "source": [
    "# Put the location of to the GGUF model that you've download from HuggingFace here\n",
    "model_path = \"models/llama-2-7b-chat.Q8_0.gguf\"\n",
    "llm = Llama(model_path=model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a280f84c-3ed1-4efc-8f72-8fd3c1b3e6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response(user_input):\n",
    "    # Prompt creation\n",
    "    system_message = \"You are a helpful assistant\"\n",
    "    user_message = user_input\n",
    "\n",
    "    prompt = f\"\"\"[INST] <<SYS>>\n",
    "    {system_message}\n",
    "    <</SYS>>\n",
    "    {user_message} [/INST]\"\"\"\n",
    "\n",
    "    # Run the model\n",
    "    output = llm(\n",
    "      prompt, # Prompt\n",
    "      max_tokens=256, # Generate up to 32 tokens\n",
    "      #stop=[\"Q:\", \"\\n\"], # Stop generating just before the model would generate a new question\n",
    "      echo=False # Echo the prompt back in the output\n",
    "    ) # Generate a completion, can also call create_completion\n",
    "\n",
    "    return(print(output[\"choices\"][0][\"text\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec32f807-207a-48f6-b2fa-84e083a07b74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vino/anaconda3/envs/ml/lib/python3.11/site-packages/llama_cpp/llama.py:1054: RuntimeWarning: Detected duplicate leading \"<s>\" in prompt, this will likely reduce response quality, consider removing it...\n",
      "  warnings.warn(\n",
      "\n",
      "llama_print_timings:        load time =    1489.04 ms\n",
      "llama_print_timings:      sample time =      13.62 ms /    39 runs   (    0.35 ms per token,  2863.23 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1488.91 ms /    34 tokens (   43.79 ms per token,    22.84 tokens per second)\n",
      "llama_print_timings:        eval time =   12568.36 ms /    38 runs   (  330.75 ms per token,     3.02 tokens per second)\n",
      "llama_print_timings:       total time =   14083.23 ms /    72 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Hello! My name is Assistant. I'm here to help you with any questions or tasks you may have, so feel free to ask me anything! 😊\n"
     ]
    }
   ],
   "source": [
    "get_response('what is your name?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec18c67d-4634-4c84-a510-07d270a58050",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1489.04 ms\n",
      "llama_print_timings:      sample time =      23.64 ms /    67 runs   (    0.35 ms per token,  2834.06 tokens per second)\n",
      "llama_print_timings: prompt eval time =     985.93 ms /    13 tokens (   75.84 ms per token,    13.19 tokens per second)\n",
      "llama_print_timings:        eval time =   21875.33 ms /    66 runs   (  331.44 ms per token,     3.02 tokens per second)\n",
      "llama_print_timings:       total time =   22904.18 ms /    79 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Of course, I'm here to help! What would you like to know or discuss? Do you have a specific question or topic in mind, or is there something you'd like me to assist you with? Please feel free to ask anything, and I'll do my best to provide helpful and accurate information.\n"
     ]
    }
   ],
   "source": [
    "get_response('can I ask you anythinhg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f967fc7-0d9c-4ef4-b78d-8e84da5502a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1489.04 ms\n",
      "llama_print_timings:      sample time =      61.55 ms /   167 runs   (    0.37 ms per token,  2713.20 tokens per second)\n",
      "llama_print_timings: prompt eval time =     914.68 ms /    13 tokens (   70.36 ms per token,    14.21 tokens per second)\n",
      "llama_print_timings:        eval time =   55301.77 ms /   166 runs   (  333.14 ms per token,     3.00 tokens per second)\n",
      "llama_print_timings:       total time =   56334.48 ms /   179 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  As a helpful assistant, I am designed to provide responses that are as informative and useful as possible. However, there is no fixed limit on the number of tokens I can output in a single response.\n",
      "I can generate text of any length, from short answers to complex explanations, depending on the context and the information requested. My training data includes a vast amount of text from various sources, which I can draw upon to provide responses that are accurate and informative.\n",
      "That being said, there may be cases where my output is limited by other factors, such as character limits or formatting constraints imposed by the platform or interface I am interacting with. In those cases, I will do my best to provide a response that is as complete and helpful as possible while still adhering to any limitations in place.\n"
     ]
    }
   ],
   "source": [
    "get_response('what is the max token you can output?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8991d79d-090e-4527-b4b5-17eff14cc4b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1489.04 ms\n",
      "llama_print_timings:      sample time =      87.30 ms /   256 runs   (    0.34 ms per token,  2932.45 tokens per second)\n",
      "llama_print_timings: prompt eval time =     839.06 ms /    14 tokens (   59.93 ms per token,    16.69 tokens per second)\n",
      "llama_print_timings:        eval time =   85494.79 ms /   255 runs   (  335.27 ms per token,     2.98 tokens per second)\n",
      "llama_print_timings:       total time =   86518.82 ms /   269 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Certainly! Here is an example of a simple PyTorch convolutional neural network (CNN) that can be used for image classification:\n",
      "```\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torchvision\n",
      "\n",
      "class Net(nn.Module):\n",
      "    def __init__(self):\n",
      "        super(Net, self).__init__()\n",
      "        self.conv1 = nn.Conv2d(1, 64, kernel_size=3, padding=1)\n",
      "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
      "        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
      "        self.fc1 = nn.Linear(9216, 128)\n",
      "        self.fc2 = nn.Linear(128, 10)\n",
      "    def forward(self, x):\n",
      "        x = self.pool(F.relu(self.conv1(x)))\n",
      "        x = self.conv2(F.relu(self.pool\n"
     ]
    }
   ],
   "source": [
    "get_response('can you give me a pytorch CNN code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3edeeade-e0b1-418e-b3c7-722e08dcafa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1489.04 ms\n",
      "llama_print_timings:      sample time =      38.66 ms /   109 runs   (    0.35 ms per token,  2819.60 tokens per second)\n",
      "llama_print_timings: prompt eval time =     427.01 ms /     8 tokens (   53.38 ms per token,    18.74 tokens per second)\n",
      "llama_print_timings:        eval time =   35939.83 ms /   108 runs   (  332.78 ms per token,     3.01 tokens per second)\n",
      "llama_print_timings:       total time =   36439.70 ms /   116 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Hello! Yes, I'm just an AI and I am trainable. My primary function is to assist and provide useful responses to users like you by learning from the data and patterns in the language I'm trained on. The more I'm used and the more data I receive, the better I can understand and respond to a wide range of questions and topics. So, feel free to teach me new words, phrases, or concepts, and I'll do my best to incorporate them into my responses!\n"
     ]
    }
   ],
   "source": [
    "get_response('are you trainable')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "90d1742d-5526-482a-8e63-5b54e89d03cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1489.04 ms\n",
      "llama_print_timings:      sample time =      63.12 ms /   165 runs   (    0.38 ms per token,  2613.86 tokens per second)\n",
      "llama_print_timings: prompt eval time =     752.92 ms /    24 tokens (   31.37 ms per token,    31.88 tokens per second)\n",
      "llama_print_timings:        eval time =   57894.83 ms /   164 runs   (  353.02 ms per token,     2.83 tokens per second)\n",
      "llama_print_timings:       total time =   58771.98 ms /   188 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Yes, I'm just an AI, I don't have a physical memory in the classical sense, but my responses and interactions are stored in a database for future reference. This means that even after our current session is closed, I can still recall our previous conversations and use them to inform future interactions.\n",
      "However, it's important to note that my training data is limited to the scope of my current training dataset, so there may be information or topics that I am not familiar with or cannot recall without additional context. Additionally, while I try my best to provide accurate and helpful responses, I'm not perfect and may make mistakes or have limitations in certain areas. If you have any specific questions or topics you would like me to remember for future interactions, feel free to let me know!\n"
     ]
    }
   ],
   "source": [
    "get_response('do you keep in memery of everthing interatec with you even i close the session now')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cc56c30e-62d6-447d-83ba-cd4b3561deb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1489.04 ms\n",
      "llama_print_timings:      sample time =      17.54 ms /    50 runs   (    0.35 ms per token,  2851.28 tokens per second)\n",
      "llama_print_timings: prompt eval time =     757.10 ms /     6 tokens (  126.18 ms per token,     7.92 tokens per second)\n",
      "llama_print_timings:        eval time =   16305.60 ms /    49 runs   (  332.77 ms per token,     3.01 tokens per second)\n",
      "llama_print_timings:       total time =   17094.72 ms /    55 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Of course, I'm here to help! How can I assist you today? Do you have any questions or tasks you need help with? Please feel free to ask me anything, and I'll do my best to assist you.\n"
     ]
    }
   ],
   "source": [
    "get_response('thank you')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e8055f-0436-48d9-b2c7-710307c42176",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
